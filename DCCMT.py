# -*- coding: utf-8 -*-
"""aipp.ipynb

Automatically generated by Colab.


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras.datasets import mnist
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Load and preprocess MNIST dataset
# Only a subset of data is used for faster execution and reduced memory usage.
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, y_train = x_train[:30000] / 255.0, y_train[:30000]  # Normalized to [0, 1]
x_test, y_test = x_test[:5000] / 255.0, y_test[:5000]
x_train = x_train[..., np.newaxis]  # Add channel dimension
x_test = x_test[..., np.newaxis]

# Visualize some sample images from the dataset
def visualize_dataset(x, y, title, num_samples=10):
    """Visualize a few samples with their labels."""
    plt.figure(figsize=(12, 4))
    for i in range(num_samples):
        plt.subplot(1, num_samples, i + 1)
        plt.imshow(x[i].squeeze(), cmap='gray')
        plt.title(f"Label: {y[i]}")
        plt.axis('off')
    plt.suptitle(title, fontsize=16)
    plt.show()

visualize_dataset(x_train, y_train, "Sample Training Data")

# Step 2: Build a memory-efficient CNN model
def build_model():
    """Builds a lightweight CNN model for MNIST classification."""
    model = Sequential([
        Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),  # Small filter size
        Flatten(),
        Dense(64, activation='relu'),  # Fewer neurons for efficiency
        Dense(10, activation='softmax')  # 10 output classes for MNIST
    ])
    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Step 3: Train the CNN model
model = build_model()
history = model.fit(x_train, y_train, epochs=3, batch_size=32, validation_split=0.1)  # Smaller batch size

# Plot the training accuracy over epochs
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("Model Training Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Step 4: Evaluate the model on clean test data
clean_accuracy = model.evaluate(x_test, y_test, verbose=0)[1]
print("Model accuracy on clean test data:", clean_accuracy)

# Step 5: Generate adversarial examples using FGSM (Fast Gradient Sign Method)
def generate_adversarial_examples(model, x, y, epsilon):
    """
    Generate adversarial examples using FGSM.
    Parameters:
        model: The trained model.
        x: Input data.
        y: True labels.
        epsilon: Perturbation magnitude.
    """
    x = tf.convert_to_tensor(x)
    y = tf.convert_to_tensor(y)
    with tf.GradientTape() as tape:
        tape.watch(x)
        predictions = model(x)
        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)
    gradients = tape.gradient(loss, x)
    adversarial_examples = x + epsilon * tf.sign(gradients)  # FGSM formula
    adversarial_examples = tf.clip_by_value(adversarial_examples, 0, 1)  # Clip values to [0, 1]
    return adversarial_examples

epsilon = 0.1  # Small perturbation magnitude
x_test_adv = generate_adversarial_examples(model, x_test, y_test, epsilon)

# Visualize the adversarial examples
visualize_dataset(x_test_adv.numpy(), y_test, "Adversarial Examples")

# Step 6: Evaluate the model on adversarial examples
adv_accuracy = model.evaluate(x_test_adv, y_test, verbose=0)[1]
print("Model accuracy on adversarial examples:", adv_accuracy)

# Step 7: Implement adversarial training
def adversarial_training(model, x_train, y_train, epsilon, epochs=3):
    """
    Perform adversarial training.
    Combines clean and adversarial data for robust training.
    """
    for epoch in range(epochs):
        adversarial_x_train = generate_adversarial_examples(model, x_train, y_train, epsilon)
        x_combined = np.concatenate((x_train, adversarial_x_train))
        y_combined = np.concatenate((y_train, y_train))
        model.fit(x_combined, y_combined, epochs=1, batch_size=32, validation_split=0.1)

adversarial_training(model, x_train, y_train, epsilon)

# Step 8: Re-evaluate the model after adversarial training
clean_accuracy_after = model.evaluate(x_test, y_test, verbose=0)[1]
adv_accuracy_after = model.evaluate(x_test_adv, y_test, verbose=0)[1]
print("Model accuracy on clean test data after adversarial training:", clean_accuracy_after)
print("Model accuracy on adversarial examples after adversarial training:", adv_accuracy_after)

# Step 9: Comparison of results
print("\nComparison of Results:")
print(f"Clean Test Accuracy: Before = {clean_accuracy:.4f}, After = {clean_accuracy_after:.4f}")
print(f"Adversarial Test Accuracy: Before = {adv_accuracy:.4f}, After = {adv_accuracy_after:.4f}")

# Plot comparison of results
def plot_comparison(before_training, after_training, categories, title):
    """Plot bar chart to compare clean and adversarial accuracies."""
    x = np.arange(len(categories))  # Label positions
    width = 0.35  # Bar width

    plt.figure(figsize=(8, 5))
    plt.bar(x - width/2, before_training, width, label='Before Training')
    plt.bar(x + width/2, after_training, width, label='After Training')
    plt.xlabel('Accuracy Categories')
    plt.ylabel('Accuracy')
    plt.title(title)
    plt.xticks(x, categories)
    plt.ylim(0, 1)
    plt.legend()
    plt.tight_layout()
    plt.show()

categories = ['Clean Accuracy', 'Adversarial Accuracy']
plot_comparison([clean_accuracy, adv_accuracy], [clean_accuracy_after, adv_accuracy_after], categories, "Model Accuracy Comparison")

# Step 10: Visualize adversarial predictions
def visualize_with_predictions(model, x, y, title, num_samples=10):
    """Visualize adversarial examples with predictions."""
    predictions = model.predict(x[:num_samples])
    predicted_labels = np.argmax(predictions, axis=1)

    plt.figure(figsize=(12, 6))
    for i in range(num_samples):
        plt.subplot(2, num_samples, i + 1)
        plt.imshow(x[i].squeeze(), cmap='gray')
        plt.title(f"True: {y[i]}")
        plt.axis('off')

        plt.subplot(2, num_samples, num_samples + i + 1)
        plt.imshow(x[i].squeeze(), cmap='gray')
        plt.title(f"Pred: {predicted_labels[i]}")
        plt.axis('off')
    plt.suptitle(title, fontsize=16)
    plt.show()

visualize_with_predictions(model, x_test_adv.numpy(), y_test, "Adversarial Examples with Predictions")